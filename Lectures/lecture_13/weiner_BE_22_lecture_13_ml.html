<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 13: Machine learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="January Weiner" />
    <meta name="date" content="2022-06-22" />
    <script src="libs/header-attrs-2.14/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="files/cubi-fonts.css" type="text/css" />
    <link rel="stylesheet" href="files/style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Lecture 13: Machine learning
]
.subtitle[
## BE_22 Bioinformatics SS 21
]
.author[
### January Weiner
]
.date[
### 2022-06-22
]

---



```
## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
```

```
## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4
## ✔ tibble  3.1.7     ✔ dplyr   1.0.9
## ✔ tidyr   1.2.0     ✔ stringr 1.4.0
## ✔ readr   2.1.2     ✔ forcats 0.5.1
```

```
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter()     masks stats::filter()
## ✖ dplyr::group_rows() masks kableExtra::group_rows()
## ✖ dplyr::lag()        masks stats::lag()
```

```
## Type 'citation("pROC")' for a citation.
```

```
## 
## Attaching package: 'pROC'
```

```
## The following objects are masked from 'package:stats':
## 
##     cov, smooth, var
```



---

## What is machine learning?

---
class:empty-slide,myinverse
background-image:url(images/person-statistics-machine-learnine-machine-learning-sand.jpeg)

---

.pull-left[

### Supervised machine learning:

 * algorithm
 * training data set
 * test data set
 * validation data set

]


---
class:empty-slide,myinverse
background-image:url(images/desert2.jpg)

.mytop[
The number of grains of sand in this picture accurately represents the
number of machine learning algorithms.
]

---

## Example: vs machine learning using PCA

The idea: 

 * select only versicolor and virginica from the iris data set
 * calculate PCA from the iris data set
 * determine a threshold value on the PC1
 * see what the results are

---

.pull-left[


```r
library(ggplot2)
library(tidyverse)
theme_set(theme_bw())
data(iris)
iris &lt;- iris %&gt;% filter(Species != "setosa") %&gt;%
  mutate(Species=factor(Species))
pca &lt;- prcomp(iris[,1:4], scale.=TRUE)
df &lt;- cbind(iris, pca$x)
ggplot(df, aes(x=PC1, y=PC2, color=Species)) + 
  geom_point()
```

]

.pull-right[

![](weiner_BE_22_lecture_13_ml_files/figure-html/iris_pca-1.svg)&lt;!-- --&gt;

]


---


.mycenter[Warning, what follows is incorrect!]


---

## INCORRECT!

.pull-left[

To discriminate, define a cutoff between the two groups.


```r
cutoff &lt;- 0.08

ml_guess &lt;- ifelse(df$PC1 &lt; cutoff, 
                   "versicolor", "virginica")
table(ml_guess, df$Species)
```

&lt;pre class="r-output"&gt;&lt;code&gt;##             
## ml_guess     versicolor virginica
##   versicolor          9        41
##   virginica          41         9
&lt;/code&gt;&lt;/pre&gt;

Error rate `\(= (11+8)/100 = 19%\)`

]


.pull-right[

![](weiner_BE_22_lecture_13_ml_files/figure-html/unnamed-chunk-4-1.svg)&lt;!-- --&gt;

]


---


## Why is that incorrect?

 * We train the model (PCA) with the full data set
 * We *test* the model with the *same* data set.
 * Testing efficiency on the data that was used to predict the model is
   *overfitting*

---

## Correct approach

Select training and testing sets:

.pull-left[

```r
set.seed(1234)
train &lt;- sample(1:nrow(iris), size = nrow(iris) * 2/3)
test  &lt;- setdiff(1:nrow(iris), train) ## remainder

model &lt;- prcomp(iris[train, 1:4], scale.=TRUE)
df_train &lt;- cbind(iris[train, ], model$x)
cutoff &lt;- -0.1 ## magic!
#ggplot(df_train, aes(x=PC1, y=PC2, color=Species)) + 
  #geom_point() + geom_vline(xintercept=cutoff)
```

]

.pull-right[

Results in training set:


```r
train_pred &lt;- ifelse(df_train$PC1 &gt; cutoff, 
                     "versicolor",
                     "virginica")
table(train_pred, df_train$Species)
```

&lt;pre class="r-output"&gt;&lt;code&gt;##             
## train_pred   versicolor virginica
##   versicolor         29         7
##   virginica           5        25
&lt;/code&gt;&lt;/pre&gt;

Error rate `\(= (7 + 5)/66 = 18%\)`
]

---

Now check in the test set:


```r
test_pred_df &lt;- data.frame(predict(model, iris[test, 1:4]))
test_pred &lt;- ifelse(test_pred_df$PC1 &gt; cutoff, 
                     "versicolor",
                     "virginica")
table(test_pred, iris[test, ]$Species)
```

&lt;pre class="r-output"&gt;&lt;code&gt;##             
## test_pred    versicolor virginica
##   versicolor         13         5
##   virginica           3        13
&lt;/code&gt;&lt;/pre&gt;

Error rate `\(= 8/34 = 24%\)`

---

## Positive vs negative: Another example

.pull-left[

 * TB patients and healthy controls
 * Metabolites as predictors
 
]

.pull-right[

![](weiner_BE_22_lecture_13_ml_files/figure-html/unnamed-chunk-8-1.svg)&lt;!-- --&gt;

]

.myfootnote[Weiner 3rd, January, et al. "Biomarkers of inflammation,
immunosuppression and stress are revealed by metabolomic profiling of
tuberculosis patients." PloS one 7.7 (2012): e40221.]

---

## Linear discriminant analysis

Idea: create a function which is a *linear combination* of the variables
(in our case, of the principal components 1 and 2).  That is,

 `$$f_{LD} = b_1 \cdot PC_1 + b_2 \cdot PC_2$$`

The coefficients are constructed in such a way that they maximize the
differences between groups on this function.

In other words, the higher the value of the function, the higher the
likelihood that the given sample belongs to a certain class.



 `$$f_{LD} = -0.18 \cdot PC_1 + 0.21 \cdot PC_2$$`

Note that the only reason we are running it on the PC variables is to have
a simple situation. Normally, you can run the LDA directly on the variables
(e.g. petal and sepal measurements from the Iris data set).

---

 `$$f_{LD} = -0.18 \cdot PC_1 + 0.21 \cdot PC_2$$`

![](weiner_BE_22_lecture_13_ml_files/figure-html/unnamed-chunk-10-1.svg)&lt;!-- --&gt;

---

## Bottom line

 * The machine algorithm generates a function of the data
 * The function gives a measure of likelihood that a certain data point is
   one of the two classes
 * We choose a threshold to determine which of the two classes this
   data point belongs to

![](weiner_BE_22_lecture_13_ml_files/figure-html/unnamed-chunk-11-1.svg)&lt;!-- --&gt;

---

For each of these situations, we can calculate the error rates.
To this end, we only use the **test data!** 
(46 samples)


```r
mtb_test &lt;- mtb_pca[test,]
mtrain_pred &lt;- predict(mod_lda, mtb_test)
mtb_test$LD &lt;- mtrain_pred$x[,1]
```


.pull-left[

**LD &gt; -1**:

&lt;pre class="r-output"&gt;&lt;code&gt;##          
## pr        HEALTHY TB
##   HEALTHY       9  0
##   TB           22 15
&lt;/code&gt;&lt;/pre&gt;

Error rate = 22 / 46 = 48  %


**LD &gt; 0**:

&lt;pre class="r-output"&gt;&lt;code&gt;##          
## pr        HEALTHY TB
##   HEALTHY      24  1
##   TB            7 14
&lt;/code&gt;&lt;/pre&gt;

Error rate = 8 / 46 = 17  %

]

.pull-right[

**LD &gt; 1**:

&lt;pre class="r-output"&gt;&lt;code&gt;##          
## pr        HEALTHY TB
##   HEALTHY      29  5
##   TB            2 10
&lt;/code&gt;&lt;/pre&gt;

Error rate = 7 / 46 = 15  %

**LD &gt; 2**:

&lt;pre class="r-output"&gt;&lt;code&gt;##          
## pr        HEALTHY TB
##   HEALTHY      31 10
##   TB            0  5
&lt;/code&gt;&lt;/pre&gt;

Error rate = 10 / 46 = 22  %

]

---

## Let us take a closer look at LD &gt; 1

**LD &gt; 1**:

&lt;pre class="r-output"&gt;&lt;code&gt;##          
## pr        HEALTHY TB
##   HEALTHY      29  5
##   TB            2 10
&lt;/code&gt;&lt;/pre&gt;

Reality is in columns, predictions in rows.

There are 2 false positives (FP) and 5
false negatives (FP). At the same time, there 
are 29 true negatives (TN) 
and 10 true positives (TP).

--


We can calculate a whole lot of different abbreviations with that!

FPR, TNR, PPV, NPV, FNR, FDR, FOR, PT, TS, ...

---
class:empty-slide,mywhite
background-image:url(images/tlas.png)

---

## Let us take a closer look at LD &gt; 1

**LD &gt; 1**:

&lt;pre class="r-output"&gt;&lt;code&gt;##          
## pr        HEALTHY TB
##   HEALTHY      29  5
##   TB            2 10
&lt;/code&gt;&lt;/pre&gt;

There are 2 false positives (FP) and 5
false negatives (FP). At the same time, there 
are 29 true negatives (TN) 
and 10 true positives (TP).

FPR describes the proportion of healthy patients that have been
misclassified:

 `$$FPR \stackrel{\text{def}}{=} \frac{FP}{FP + TN}$$`

In our case, FPR = 2 / 31 = 6 %

--

Similarly, FNR is the false negative rate and describes the proportion of
tuberculosis patients that have been classified as healthy.

 `$$FNR \stackrel{\text{def}}{=} \frac{FN}{FN + TP}$$`

In our case, FPR = 5 / 15 = 33 %


---

## Sensitivity and specificity 


.pull-left[

**LD &gt; -1**:

&lt;pre class="r-output"&gt;&lt;code&gt;##          
## pr        HEALTHY TB
##   HEALTHY       9  0
##   TB           22 15
&lt;/code&gt;&lt;/pre&gt;

All TB patients have been classified as positive! Very sensitive. However, 
specificity is poor: 21 of the healthy individuals have been classified as
TB.


**LD &gt; 0**:

&lt;pre class="r-output"&gt;&lt;code&gt;##          
## pr        HEALTHY TB
##   HEALTHY      24  1
##   TB            7 14
&lt;/code&gt;&lt;/pre&gt;

Slightly less sensitive, but way more specific.

]

.pull-right[

**LD &gt; 1**:

&lt;pre class="r-output"&gt;&lt;code&gt;##          
## pr        HEALTHY TB
##   HEALTHY      29  5
##   TB            2 10
&lt;/code&gt;&lt;/pre&gt;

Way less sensitive, but much more specific.


**LD &gt; 2**:

&lt;pre class="r-output"&gt;&lt;code&gt;##          
## pr        HEALTHY TB
##   HEALTHY      31 10
##   TB            0  5
&lt;/code&gt;&lt;/pre&gt;

Best specificity so far (no FPs at all!), but really poor sensitivity.

]

---

## Formal definitions

.pull-left[

### Sensitivity

 * recall, TPR, probability of detection, power

 `$$SEN = \frac{TP}{TP + FN} = \frac{TP}{P} = 1 - FNR$$`

(where P is the total number of real patients)

This is the fraction of the patients that we were able to detect.

Low sensitivity `\(\rightarrow\)` many patients misclassified as healthy
individuals.

]

.pull-right[

### Specificity

 * TNR, selectivity

 `$$SPC = \frac{TN}{N} = 1 - FPR$$`

This is the fraction of healthy individuals which we recognized as healthy.

Low specificity `\(\rightarrow\)` many healthy individuals classified as
patients.

]


---

## What if we plot SPC vs SEN for our data?

![](weiner_BE_22_lecture_13_ml_files/figure-html/unnamed-chunk-23-1.svg)&lt;!-- --&gt;

---

## Receiver-operator characteristic curves

.pull-left[

![](weiner_BE_22_lecture_13_ml_files/figure-html/unnamed-chunk-24-1.svg)&lt;!-- --&gt;

]

.pull-right[

*Area under the curve*,
AUC = 0.94 [95% CI 0.87-1.00]

]

---

## Cross-validation

Splitting data set into training and test set:

 * we cannot build prediction based on the same data that was used to
   construct the model (**overfitting**)
 * training set small, so learning efficiency takes a hit
 * test set small, so errors in estimation (confidence intervals large)

Solution: cross-validation


---

## LOO (leave-one-out)



```
# pseudocode

for i in samples:
  train := samples[ -i ]
  test  := samples[  i ]
  model := algorithm(train)
  prediction := model(test)
  record(i, prediction)
```

In each iteration, we take out *one* sample. We then use all the remaining
samples to create the model, we apply it to the one remaining sample (which
is our test), and we record that prediction.

After all samples are through, we have as many predictions as we have total
samples, but we avoided overfitting.

---

## K-fold cross-validation

Pseudocode:

```
folds := split(data, 10)
for i in 1:10:
  train := folds[-i]
  test  := folds[i]
  model := algorithm(train)
  prediction := model(test)
  record(i, prediction)
```

We split the data in e.g. 10 subsets = folds. For each fold, we treat it as
a test set, and  we use all the remaining folds to create a model. We then
apply the model to the test fold and record the predictions.

After all folds have been analysed we have no overfitting, but we have
predictions for all samples.


---

## Bias-variance tradeoff

Two important concepts:

 * **bias** results from model being inaproppriate for the data (or too
   simple) – that is, model is underfitting the data
 * **variance** results from model being too close to the training data (or
   too complex) – that is, model is overfitting the data

![](images/bias_variance_2.png)

---

## Bias-variance tradeoff

![](images/bias_variance.png)


---

## Bias-variance tradeoff

![:scale 70%](images/bias_variance_3.png)

 






---
class:empty-slide,mywhite
background-image:url(images/machine_learning.png)


---

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="files/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

---
title: "Lecture 8: Gene expression analysis"
subtitle: "BE_22 Bioinformatics SS 21"
author: "January Weiner"
date: "`r Sys.Date()`"
outputdupa: html_document
output:
  xaringan::moon_reader:
    self-contained: true
    css: ["default", "files/cubi-fonts.css", "files/style.css" ]
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "files/macros.js"
toc: no
---

```{r,echo=FALSE}
## Set default options for the knitr RMD processing
knitr::opts_chunk$set(echo=FALSE,warning=FALSE,message=FALSE,fig.width=5,fig.height=5,cache=FALSE,autodep=TRUE, results="hide")
library(pander)
library(knitr)
library(kableExtra)
#source("functions.R")
```

```{r echo=FALSE,include=FALSE,eval=TRUE}
options(crayon.enabled = TRUE)
options(crayon.colors = 256)
knitr::knit_hooks$set(output = function(x, options){
  paste0(
    '<pre class="r-output"><code>',
    fansi::sgr_to_html(x = htmltools::htmlEscape(x), warn = FALSE),
    '</code></pre>'
  )
})

## this is an ugly, ugly hack, but otherwise crayon does not LISTEN TO REASON!!!
num_colors <- function(forget=TRUE) 256
library(crayon)
assignInNamespace("num_colors", num_colors, pos="package:crayon")
```


```{r libraries,cache=FALSE}
library(tidyverse)
library(ggplot2)
library(readxl)
library(cowplot)
library(RColorBrewer)
library(cluster)
library(Rseasnap)
library(factoextra)
library(pca3d)
library(colorDF)
```

<!--
class:empty-slide,myinverse
background-image:url(images/arnolfini.jpg)
-->


## Plan for today

 * Gene expression analysis – as an in-depth example of high-throughput
   analyses

---

## Transcriptomic methods

 * QPCR: precise, low-throughput
 * Nanostring: precise, mid-throughput (~ 500-1000 genes)
 * Microarray: less exact, high-throughput, pre-defined genes
 * RNA-Seq: very flexible, less exact, high-throughput

---

## QC – quality control

 * Method-dependent quality measures
   * aligned reads
   * duplications
   * intron / exon binding
   * Y chromosome binding
   * adapter content
 * Bias? Clustering?

---

## (Demo) Example RNA-Seq QC

Document derived using the `multiQC` package can be retrieved
[here](multiqc.all_samples.all_mates.qc_report.html)

---

## The GSE156063 data set

```{r}
lcpm <- readRDS("../../Datasets/lcpm_sel.rds")
covar <- readRDS("../../Datasets/covar.rds")
```

```{r results="markdown"}
library(kableExtra)
knitr::kable(summary_colorDF(covar))
```

---

## The GSE156063 data set

```{r results="markdown"}
library(kableExtra)
knitr::kable(summary_colorDF(covar)) %>%
  row_spec(7:10, bold=TRUE, color="white", background = "#D7261E")
```


---

How do the count data look like?


```{r results="markdown"}
cnts <- readRDS("../../Datasets/counts.rds")
cnts %>% as.data.frame %>% 
  dplyr::select(1:6) %>% slice(1:10) %>%
             mutate(' ... '='...')  %>%
knitr::kable()

```

---

# QC using PCA

Dimension reduction with PCA allows us to see batch effects and also have a
first preliminary look at the data.


---


```{r pca_principle}
x <- rnorm(120)
y <- 0.2 * (x + rnorm(120))
#plot(x, y)
pca <- prcomp(cbind(x, y), scale.=FALSE)
#plot(pca$x[,1], pca$x[,2])
df <- data.frame(x=x, y=y, PC1=pca$x[,1], PC2=pca$x[,2]) %>%
  arrange(x) %>% mutate(col=1:length(x))
```

.pull-left[

### Before PCA:

```{r}
theme_set(theme_bw())
ggplot(df, aes(x, y, color=col)) + geom_point(show.legend=FALSE) + scale_color_viridis_c()
```

Correlation between x and y = `r sprintf("%.2f", cor(x, y))`

]

.pull-right[

### After PCA:

```{r}
theme_set(theme_bw())
ggplot(df, aes(PC1, PC2, color=col)) + geom_point(show.legend=FALSE) + scale_color_viridis_c()
```

Correlation between PC1 and PC2 = `r sprintf("%.2f", cor(df$PC1, df$PC2))`
]

---

```{r pca_principle2}
x <- rep(c(-1, 1), each=120) + rnorm(240)
y <- 0.2 * (x + rnorm(240))
#plot(x, y)
pca <- prcomp(cbind(x, y), scale.=FALSE)
#plot(pca$x[,1], pca$x[,2])
df <- data.frame(x=x, y=y, PC1=pca$x[,1], PC2=pca$x[,2]) %>%
  mutate(col=rep(c("A", "B"), each=120))
```

.pull-left[

### Before PCA:

```{r}
theme_set(theme_bw())
ggplot(df, aes(x, y, color=col)) + geom_point(show.legend=FALSE) + scale_color_brewer(palette = "Dark2")
```

Correlation between x and y = `r sprintf("%.2f", cor(x, y))`

]

.pull-right[

### After PCA:

```{r}
theme_set(theme_bw())
ggplot(df, aes(PC1, PC2, color=col)) + geom_point(show.legend=FALSE) + scale_color_brewer(palette = "Dark2")
```

Correlation between PC1 and PC2 = `r sprintf("%.2f", cor(df$PC1, df$PC2))`
]

---

.pull-left[

### Before PCA:

```{r}
g1 <- ggplot(df, aes(x=x, fill=col, group=col)) + geom_density(alpha=.2) + scale_fill_brewer(palette = "Dark2") +
  ggtitle("Coordinate X")
g2 <- ggplot(df, aes(x=y, fill=col, group=col)) + geom_density(alpha=.2) + scale_fill_brewer(palette = "Dark2") +
  ggtitle("Coordinate Y")
plot_grid(g1, g2, ncol=1)
```

]

.pull-right[

### After PCA:

```{r}
g1 <- ggplot(df, aes(x=PC1, fill=col, group=col)) + geom_density(alpha=.2) + scale_fill_brewer(palette = "Dark2") +
  ggtitle("Coordinate PC1")
g2 <- ggplot(df, aes(x=PC2, fill=col, group=col)) + geom_density(alpha=.2) + scale_fill_brewer(palette = "Dark2") +
  ggtitle("Coordinate PC2")
plot_grid(g1, g2, ncol=1)
```
]

---

```{r pca_lcpm, results="asis",fig.width=12,fig.height=8}
pca <- prcomp(t(lcpm), scale.=TRUE)
df <- data.frame(covar, pca$x[,1:4])
plot_ly_pca(pca$x, covar %>% select(group, gender, disease_state, age))
```

---

## PCA "scree plot"

```{r}
v <- pca$sdev^2
v <- v/sum(v)
df <- data.frame(PC=colnames(pca$x), Variance=v)[1:9, ]
ggplot(df, aes(x=PC, y=Variance, group=1)) + geom_point() + geom_line() +
  ggtitle("PCA scree plot") + ylab("Percentage of variance explained")
```



---

## Heatmaps of the most variable genes

```{r fig.width=12,fig.height=6}
library(gplots)
library(RColorBrewer)
lcpm_sel <- lcpm[1:50, ]
colnames(lcpm_sel) <- paste(covar$group, covar$gender)
heatmap.2(lcpm_sel, trace="n",
  ColSideColors=brewer.pal(4, "Dark2")[ as.numeric(as.factor(covar$group))])
```




---

## Differential gene expression analysis

 * Finding differences between groups
 * Models may vary

---

## GLMs – generalized linear models
 
 * OLS – ordinary least squares regression
 * Fit data (independent + dependent variables) to a model – a bit like in
   OLS
 * Differences to OLS
   * **link function** allows models in which variables have
     non-normal distributions
   * maximum likelihood estimation (rather than least squares)
 * Count data: negative binomial distribution of the dependent variables

---

## GLMs – generalized linear models

For a given gene, we assume that its counts in a sample $i$ have negative
binomial distribution:

 $$\text{counts} \sim f_{\text{binom}}(\mu_i, \alpha)$$

(Negative binomial distribution is a generalization of a Poisson
distribution that you might know)

Statistical test:

 **Null hypothesis $H_0$:** For each sample $i$, $\mu_i = \mu_0$

 **Alternative hypothesis $H_1$:** 

 $$\log{\mu_i} = \beta_0 + x_i \times \beta_G$$

Where 


 $$\beta_G = \left\{\begin{array}{ll}0 & x_i \text{is control} \\ 1 & x_i \text{is treatment}\end{array}\right.$$

This model is then tested against the $H_0$ using a Wald test.


---

## Correction for multiple testing

.pull-left[

Statistical testing is like a russian roulette: each time you test, you
have a chance of drawing a False Positive (FP). If you draw many times, you
are bound to have "significant" FP's. 

 * Family-wise error correction (e.g. Bonferroni): guarantees that the
   chance of having 1 or more FP's in the results significant at $\alpha$
   is $\alpha$.
 * False-discovery rate (FDR; e.g. Benjamini-Hochberg): guarantees that the
   *fraction* of FP's in the results significant at $\alpha$ is no more
   than $\alpha$.

]

.pull-right[

 **Remember:** any results not corrected for multiple testing are *not
 trustworthy!* 

]


---

## R equation language

R uses a particular representation of linear model equations:

e.g. 

 $$y = \beta_0 + \beta_1 \times y$$

is represented as

 `y ~ x`

Whereas 

 $$y = \beta_1 \times y$$

(no intercept, i.e. intercept $= 0$)

becomes

 `y ~ 0 + x`


---

```{r results="markdown"}
res <- readRDS("../../Datasets/sc2_res.rds")
options(width=80)
head(res)
```

---

## Heatmap of the DE genes

```{r fig.width=12,fig.height=6}
sel <- res$ENSEMBL[1:50]
lcpm_sel <- lcpm[sel, ]
colnames(lcpm_sel) <- paste(covar$group, covar$gender)
heatmap.2(lcpm_sel, trace="n",
  ColSideColors=brewer.pal(4, "Dark2")[ as.numeric(as.factor(covar$group))])
```

---

## Plotting individual genes

.pull-left[

```{r}
library(ggbeeswarm)
cnts <- readRDS("../../Datasets/counts_sel.rds")
id <- "ENSG00000126709"
df <- data.frame(covar, Counts=cnts[id, ])
ggplot(df, aes(x=covar$disease_state, y=Counts)) + geom_boxplot(outlier.shape=NA) + 
  geom_beeswarm() + xlab("Disease state")
```
]

--

.pull-right[


```{r}
df <- data.frame(covar, lCPM=lcpm_sel[id, ])
ggplot(df, aes(x=covar$disease_state, y=lCPM)) + geom_boxplot(outlier.shape=NA) + 
  geom_jitter() + xlab("Disease state")
```

]

---

## What next?

 * What is our research question?
 * How do we get from genes to functions or pathways?
 * How do we validate our data?


---

```{r data loading etc.,cache=TRUE}
rgs <- readRDS("../../Datasets/rgs.rds")
.vars <- apply(rgs$E, 1, var)
rgsS <- rgs[ .vars > quantile(.vars, .99), ]
.cors <- cor(t(rgsS$E))
```

# Clustering

 * "unbiased" machine learning
 * group data points (samples, genes, ...) by similarity
 * applications in many areas of bioinformatics

---

.pull-left[
## Why cluster the genes?

 * Genes which are co-regulated have similar function $\rightarrow$
   identification of "transcriptional modules"
 * Analysis of the clusters may give us clues about what is happening in
   the biological system
 * Better visualization if genes are clustered

]

.pull-right[

```{r fig.width=5,fig.height=10,results="markdown",out.width="60%",cache=TRUE}
gn1 <- "FCGR1B"
gn2 <- "ANKRD22"
gn3 <- "OR51A4"

id1 <- rgs$genes$ProbeName[ match(gn1, rgs$genes$GeneName) ]
id2 <- rgs$genes$ProbeName[ match(gn2, rgs$genes$GeneName) ]
id3 <- rgs$genes$ProbeName[ match(gn3, rgs$genes$GeneName) ]

theme_set(theme_bw())

df <- data.frame(rgs$targets, g1=rgs$E[id1, ], g2=rgs$E[id2, ], g3=rgs$E[id3, ])

g1 <- ggplot(df, aes(x=g1, y=g2, color=ARM, symbol=Timepoint)) + geom_point(color="#33333366") +
  scale_color_brewer(palette = "Dark2") + xlab(gn1) + ylab(gn2)
g2 <- ggplot(df, aes(x=g1, y=g3, color=ARM, symbol=Timepoint)) + geom_point(color="#33333366") +
  scale_color_brewer(palette = "Dark2") + xlab(gn1) + ylab(gn3)
plot_grid(g1, g2, nrow=2)
```
]

---
class:empty-slide,mywhite
background-image:url(images/network_1.png)


---
class:empty-slide,mywhite
background-image:url(images/network_2.png)

---

### From correlations to distances

 $d_{i,j}$ – Distance between genes $i$ and $j$

 $\rho(x_i, x_j)$ – correlation coefficient between expression of $i$ $x_i$ and $j$ $x_j$

Hard threshold:

 $$d_{i,j} \equiv \left\{\begin{array}{ll}0 & |\rho(x_i, x_j)| > \tau\\1 & |\rho(x_i, x_j)| \leq \tau\end{array}\right.$$

Continuouos in $(0, 1)$

 $$d_{i,j} \equiv 1 - |\rho(x_i, x_j)|$$


---
class:empty-slide,mywhite
background-image:url(images/correlation_modules.png)


---
class:empty-slide,mywhite
background-image:url(images/correlation_dependence.png)



---

## Many alternatives

 Other measures exist:

   * mutual information
   * different types of correlation (Spearman $\rho$, Kendall $\tau$)
   * distance correlation (Székely 2007)


Bottom line: we need to get at distances

---
class:empty-slide,myinverse
background-image:url(images/desert2.jpg)

.mytop[
The number of grains of sand in this picture accurately represents the
number of clustering algorithms.
]

--
.mytop[
<br/>
<br/>
<br/>
<br/>
<br/>
Each tree in the picture corresponds to a method of clustering which is
robust, reliable and provides automatically high quality clusters. 
]



---
class:empty-slide,myinverse
background-image:url(images/desert2.jpg)

.pull-left[
.transpblock[
**PAM** Partitioning Around Medoids. Similar to k-means;
predefined number of clusters
]

.transpblock[
**SOMs**, Self-organizing maps:
Train a neural network to recognize clusters in the data
(also SOTA: self-organizing trees)
]

.transpblock[
**SVC** – support vector clustering;
based on SVM’s (support vector machines)
]


.transpblock[
**Fuzzy C-means clustering**:
Each sample gets assigned a probability of
belonging to each of the clusters.
Algorithm similar to k-means.
Also similar – “soft k-means”
]

]

.pull-right[

.transpblock[
**Hierarchical clustering:** UPGMA (group-average linkage), Ward's method, neighbor joining, single
linkage, complete linkage...

]

.transpblock[
**Mclust** – model based clustering
The data is fit to a statistical model consisting of K normal distributions.
]

.transpblock[
**Density based clustering:**
identify clusters by local density profiles
]

]

---

### Hierarchical clustering

 * Start with each element in its own cluster: $n_{\text{clust}} = N$
 * Identify two elements with the smallest distance
 * Recalculate distances using a *linkage* function

E.g. complete linkage:

 $$D(C_i, C_j) = \max_{e_i \in C_i, e_j \in C_j} D(e_i, e_j)$$

Basically: join the clusters with the smalles distance between the
*furthest* elements of the clusters

E.g. single linkage:

 $$D(C_i, C_j) = \min_{e_i \in C_i, e_j \in C_j} D(e_i, e_j)$$

Basically: join the clusters with the smalles distance between the
*closest* elements of the clusters

---
class:empty-slide,mywhite
background-image:url(images/hclust.png)


---

## k-means

Naive k-means:

 * Assign the elements to random clusters
 * Repeat:
   * Calculate the cluster centroids (midpoints) 
   * Assign the elements to the cluster with the closest centroid
   * Abort if the asignments no longer change

```{r}
# generates random clusters
get_clusts <- function(centr, sds, N) {
  clusts <- map2(centr, sds, ~ {
                   cc <- .x
                   sd <- .y
                   ret <- map(cc, ~ rnorm(N, mean=.x, sd=sd)) %>% reduce(cbind)
                   colnames(ret) <- paste0("V", 1:ncol(ret))
                   ret
     })
  clusts <- imap_dfr(clusts, ~ data.frame(.x, Cl=.y))

  clusts$KM <- sample(names(centr), nrow(clusts), replace = TRUE)
  return(clusts)
}

centres <- list(A=c(0,0), B=c(-1,1), C=c(1,1))
sds   <- c(.5, .5, .5)
clusts <- get_clusts(centres, sds, 55)
cldist <- function(x, y) {
  ## y is a matrix, x is a vector


}
gplots <- list()

for(i in 1:10) {
  gplots[[i]] <- ggplot(clusts, aes(V1, V2, color=KM)) + geom_point()
  centr <- clusts %>% group_by(KM) %>% 
    summarise(across(starts_with("V"), mean)) 
  mtx <- rbind(
               data.matrix(centr %>% select(starts_with("V"))),
               data.matrix(clusts %>% select(starts_with("V")))
               )
  dists <- dist(mtx) %>% as.matrix 
  dists <- dists[ -1:-3, 1:3 ]

  clusts <- clusts %>% mutate(
                            KM=centr[["KM"]][ apply(dists, 1,
                                                    which.min)
                            ])
}

```

.center[
```{r fig.width=8,fig.height=5}
gplots[1]
```
]

---


.center[
```{r fig.width=8,fig.height=5,animation.hook="gifski",cache=TRUE}
for(i in 1:length(gplots)) {
  print(gplots[[i]] + ggtitle(paste0("Step: ", i)))
}
```
]

---

## Measuring clustering performance

.pull-left[

 * How many clusters?

 * Which clustering method?

 * What parameters?
]

--

.pull-right[

 * Internal: information scientific

 * Stability measures: cross-validation, resampling (like bootstrapping in
   phylogenies!)

 * External: use external information
   * a priori information
   * biological information (how well the clusters correspond to
      underlying biology)

]

---
class:empty-slide,myinverse
background-image:url(images/desert2.jpg)

--

.mytop[
The number of grains of sand in this picture accurately represents the
number of available measures of clustering performance.
]


---

## The elbow method

.pull-left[
There is a total amount of variance in the clusters.

We can split it into *between cluster* variance and *within cluster*
variance.

Instead of variance, we use Sum of Squares (SS), because $Var(x)\equiv \frac{SS}{n-1}$.

 $$SS_{\text{tot}} = SS_{\text{within}} + SS_{\text{between}}$$

 * If all elements are in one large cluster, $SS_{\text{between}}= 0$.
 * If all elements are each in a separate cluster, $SS_{\text{within}}= 0$.
]

.pull-right[


![](images/elbow.png)


]

---

## Silhouette plots

.pull-left[

For each element, we plot the difference between distance to the center of
the cluster where that element is and the distance to the closest *other*
cluster center.

]

.pull-right[

```{r}
cl <- list(
clusts1=get_clusts(centres, c(.2, .2, .2), 20),
clusts2=get_clusts(centres, c(.7, .7, .7), 20),
clusts3=get_clusts(centres, c(1.5, 1.5, 1.5), 20))

mtx <- map(cl, ~ as.matrix(.x[, c("V1", "V2")]))
#km <- map(mtx, ~ kmeans(.x, 3)$cluster)
km <- map(cl, ~ as.numeric(factor(.x[["Cl"]])))
dd <- map(mtx, ~ dist(.x))
sl <- map2(km, dd, ~ silhouette(.x, .y))
pal <- brewer.pal(3, "Dark2")
```

```{r}
theme_set(theme_bw())
fviz_silhouette(sl[[1]]) + coord_flip() + 
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2")
```


]

---

```{r fig.width=14,fig.height=7}
i <- 1
g1 <- ggplot(cl[[i]], aes(V1, V2, color=Cl)) + geom_point() +
  scale_color_brewer(palette = "Dark2") + stat_ellipse()
g2 <- fviz_silhouette(sl[[i]]) + coord_flip() + 
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2")
plot_grid(g1, g2)
```



---

```{r fig.width=14,fig.height=7}
i <- 2
g1 <- ggplot(cl[[i]], aes(V1, V2, color=Cl)) + geom_point() +
  scale_color_brewer(palette = "Dark2") + stat_ellipse()
g2 <- fviz_silhouette(sl[[i]]) + coord_flip() + 
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2")
plot_grid(g1, g2)
```



---

```{r fig.width=14,fig.height=7}
i <- 3
g1 <- ggplot(cl[[i]], aes(V1, V2, color=Cl)) + geom_point() +
  scale_color_brewer(palette = "Dark2") + stat_ellipse()
g2 <- fviz_silhouette(sl[[i]]) + coord_flip() + 
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2")
plot_grid(g1, g2)
```



---

Another example: clustering of single cell data

![](images/cell_1.png)

---

Another example: clustering of single cell data

![](images/cell_2.png)


---

Another example: clustering of single cell data

![](images/cell_3.png)
